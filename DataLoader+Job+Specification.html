<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    <head>
        <title>Hadoop : DataLoader Job Specification</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body>
        <div id="page">
            <div id="main">
                <div id="main-header" class="pageSectionHeader">
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Hadoop : DataLoader Job Specification
                        </span>
                    </h1>

                    <div class="page-metadata">
                        <p>This page last changed on Aug 29, 2013 by <font color="#0050B2">beckmj</font>.</p>
                    </div>
                </div>

                <div id="content" class="view">
                    <div id="main-content" class="wiki-content group">
                    <h1 id="DataLoaderJobSpecification-WhatisaJobSpecification(Jobspec)?">What is a Job Specification (Jobspec)?</h1><p>A Job Specification or JobSpec (also known as a Transfer Spec) is a hierarchal XML-based file that defines a job being submitted. It is called by the dataloader-cli.sh submit -i &lt;transfer specification&gt; command.</p><p>Refer to the “File Spec Samples”  for examples of the use of the elements and attributes used in these files.</p><p>The “JobSpec Elements and Attributes” table below shows the elements and attributes of each of these types.</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Element</th><th class="confluenceTh">Element attribute</th><th class="confluenceTh">Element value</th></tr><tr><td class="confluenceTd"><p><strong>name</strong>: The name is the<br />name of the job.</p></td><td class="confluenceTd"> </td><td class="confluenceTd"> </td></tr><tr><td class="confluenceTd"><strong>type</strong>: the type can be batch or streaming</td><td class="confluenceTd">batch or streaming</td><td class="confluenceTd"> </td></tr><tr><td class="confluenceTd"><strong>streaming</strong>: if the type is &quot;streaming&quot;, this element should be set</td><td class="confluenceTd"><strong>type</strong>: this is an attribute of the streaming element</td><td class="confluenceTd"> available values are: push, pull</td></tr><tr><td class="confluenceTd"> </td><td class="confluenceTd"><strong>feedSet</strong>: this element specifies the content to be copied; each feedSet includes data in one datastore; this element can occur many times. </td><td class="confluenceTd"><strong>d</strong><span style="line-height: 1.4285715;"><strong>atastore</strong>: type URI, specifies the datastore - can occur only once.</span><p><strong>feed</strong>: can occur multiple times. It can have only one attribute: path, which specifies the actual path to be copied.</p></td></tr><tr><td class="confluenceTd"> </td><td class="confluenceTd"><strong>destination</strong>: this element can occur only once. It specifies the destination of the streaming job datastore: this element is of type URI, specifies the target datastore; can occur only once. </td><td class="confluenceTd"> </td></tr><tr><td class="confluenceTd"> </td><td class="confluenceTd"><strong>path</strong>: string element. Specifies the location on the target datastore where data is to be copied. In Pivotal HD1.0, can be used only once.</td><td class="confluenceTd"> </td></tr><tr><td class="confluenceTd"> </td><td class="confluenceTd"><strong>reader</strong>: element of type &quot;ReaderType&quot;. Specifies the reader that will be used to read data from the source datastore. Can occur only once.</td><td class="confluenceTd"><strong>property</strong>: a key-value pair used as the reader properties. Can occur multiple times.</td></tr><tr><td class="confluenceTd"> </td><td class="confluenceTd"><strong>pipeline</strong>: this element specifies the pipeline that will be used</td><td class="confluenceTd"><p><strong>worker</strong>: this attribute specifies the worker that will be used in the pipeline; can occur many times; <br /><strong>name</strong>: the name of the worker; <br />order: int value, the order of this worker in the pipeline; value should not be the same</p></td></tr><tr><td class="confluenceTd"> </td><td class="confluenceTd"><strong>writer</strong>: this element is of type &quot;WriterType&quot;, specifies the writer that will be used to write data to the target datastore; can occur only once</td><td class="confluenceTd"><strong>property</strong>: a key-value pair used as the reader properties. Can occur multiple times.</td></tr><tr><td class="confluenceTd">batch: if the type is batch, this element should be set</td><td class="confluenceTd"><strong>fileSet</strong>: this element specifies the files/items to be copied; each fileSet includes data in one datastore; this element can occur many times</td><td class="confluenceTd"><p><strong>datastore</strong>: URI value, specifies the source <strong>datastore</strong>; can occur only once; <br /><strong>filePath</strong>: this element is of type &quot;FilePathType&quot;, specifies one path that needs to be copied, can occur many times <br />globString: glob string used to filter path, only valid if the type if &quot;glob&quot; <br /><strong>filePath</strong>: recursive value of the same type &quot;FilePathType&quot;, provides more detailed file listing function<br /><strong>filter</strong>: attribute, specifies the file name filter <br /><strong>type</strong>: attribute, the type of this path, available values are: &quot;file&quot;, &quot;folder&quot;, &quot;glob&quot;, &quot;list&quot; <br /><strong>path</strong>: attribute, the exact file path that this filePath specifies</p><p><strong>disk</strong>: attribute, the disk that this filePath resides on, only valid if the datastore is localfs</p></td></tr></tbody></table></div><p> </p><h2 id="DataLoaderJobSpecification-FileSpecSamples">File Spec Samples</h2><p><strong>Sample ftp.xml file with file entry type &quot;glob&quot;:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;&quot;&gt;
	&lt;name&gt;batch-job-1&lt;/name&gt;
	&lt;type&gt;batch&lt;/type&gt;
	&lt;batch&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;ftp://YOUR_SOURCE_HOSTNAME:21/&lt;/datastore&gt;
			&lt;filePath type=&quot;glob&quot; path=&quot;/&quot; &gt;
				&lt;globString&gt;/movie/**/*.MOV&lt;/globString&gt;
			&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://TARGET_HDFS_NAMENODE:8020/data&lt;/datastore&gt;
			&lt;path type=&quot;folder&quot; path=&quot;/2013/01&quot;&gt;&lt;/path&gt;
			&lt;trimPrefix&gt;/movie&lt;/trimPrefix&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;FS_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;FS_WRITER&quot;&gt;
		&lt;/writer&gt;
	&lt;/batch&gt;
&lt;/JobSpec&gt;</pre>
</div></div><p>Note: For the ftp.xml transfers, filenames that end with &quot;.MOV&quot; from the source ftp://&lt;YOUR_SOURCE_HOSTNAME:21/ maintain the folder structure, but the &quot;/movie&quot; is trimed off from the folder hierarchy. Sub-folders of the movie folder will be copied to the destination HDFS hdfs://&lt;TARGET_HDFS_NAMENODE&gt;:8020/data/2013/01</p><p><strong>Sample localfs.xml:</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;&gt;
	&lt;name&gt;localfs job&lt;/name&gt;
	&lt;type&gt;batch&lt;/type&gt;
	&lt;batch&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;localfs://YOUR_SLAVE1_HOSTNAME/datastore_rootpath&lt;/datastore&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs&quot; filter=&quot;*.log&quot;&gt;
			&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;localfs://YOUR_SLAVE2_HOSTNAME/datastore_rootpath&lt;/datastore&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs&quot; filter=&quot;*.log&quot;&gt;
			&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://YOUR_TARGET_NAMENODE:8020/data&lt;/datastore&gt;
			&lt;path type=&quot;folder&quot; path=&quot;/2013/01&quot;&gt;&lt;/path&gt;
			&lt;trimPrefix&gt;/logs&lt;/trimPrefix&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;FS_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;FS_WRITER&quot;&gt;
		&lt;/writer&gt;
	&lt;/batch&gt;
&lt;/JobSpec&gt;
</pre>
</div></div><p>This spec will transfer the files in the folder &quot;/logs&quot; on the two specified localfs datastore to the target HDFS. </p><p><strong>Sample localfs_disk.xml</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;
	xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
	xsi:schemaLocation=&quot;http://www.pivotal.com/gphd/dataloader ../../../main/xsd/JobSpec.xsd &quot;&gt;
	&lt;name&gt;localfs job&lt;/name&gt;
	&lt;type&gt;batch&lt;/type&gt;
	&lt;batch&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;localfs://YOUR_SLAVE1_HOSTNAME/datastore_rootpath&lt;/datastore&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs/data1&quot; filter=&quot;*.log&quot; disk=&quot;disk1&quot;&gt;
			&lt;/filePath&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs/data2&quot; filter=&quot;*.log&quot; disk=&quot;disk2&quot;&gt;
			&lt;/filePath&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs/data3&quot; filter=&quot;*.log&quot; disk=&quot;disk3&quot;&gt;
			&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;localfs://YOUR_SLAVE2_HOSTNAME/datastore_rootpath&lt;/datastore&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs&quot; filter=&quot;*.log&quot; disk=&quot;disk1&quot;&gt;
			&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://YOUR_TARGET_NAMENODE:8020/data&lt;/datastore&gt;
			&lt;path type=&quot;folder&quot; path=&quot;/2013/01&quot;&gt;&lt;/path&gt;
			&lt;trimPrefix&gt;/logs&lt;/trimPrefix&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;FS_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;FS_WRITER&quot;&gt;
		&lt;/writer&gt;
	&lt;/batch&gt;
&lt;/JobSpec&gt;
</pre>
</div></div><p>This specification specifies the similar data to be transferred with the one above but with the disk information added. This information will be useful when using  localfs strategy and you set WORKERS_PER_DISK in the transfer policy. See the <em>DataLoader Command Line Reference</em> for more detail. In the sample, YOUR_SLAVE_HOSTNAME is the hostname of a slave machine. Note that multiple disks can reside on the same host, but must be different physical disks.</p><p><strong>Sample nfs.xml file with file entry type &quot;folder&quot;</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;&gt;
	&lt;name&gt;nfs job&lt;/name&gt;
	&lt;type&gt;batch&lt;/type&gt;
	&lt;batch&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;nfs://NFS_SERVER_HOST/datastore_rootpath&lt;/datastore&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs&quot; filter=&quot;*.log&quot;&gt;&lt;/filePath&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs2&quot; filter=&quot;*.log&quot;&gt;&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://YOUR_TARGET_NAMENODE:8020/data&lt;/datastore&gt;
			&lt;path type=&quot;folder&quot; path=&quot;/2013/01&quot;&gt;&lt;/path&gt;
			&lt;trimPrefix&gt;/logs&lt;/trimPrefix&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;FS_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;FS_WRITER&quot;&gt;
		&lt;/writer&gt;
	&lt;/batch&gt;
&lt;/JobSpec&gt;
</pre>
</div></div><p>The specification file transfers folders /logs and /log2 from the source <a href="nfs://NFS_SERVER_HOST/datastore_rootpath" rel="nofollow">nfs://NFS_SERVER_HOST/datastore_rootpath</a> to the destination <a href="hdfs://YOUR_TARGET_NAMENODE:8020/data" rel="nofollow">hdfs://YOUR_TARGET_NAMENODE:8020/data</a>.</p><p><strong>Sample hdfs.xml</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;&gt;
	&lt;name&gt;hdfs job&lt;/name&gt;
	&lt;type&gt;batch&lt;/type&gt;
	&lt;batch&gt;
		&lt;fileSet&gt;
			&lt;datastore&gt;hdfs://YOUR_SOURCE_NAMENODE:8020/datastore_rootpath&lt;/datastore&gt;
			&lt;filePath type=&quot;folder&quot; path=&quot;/logs&quot; filter=&quot;*.log&quot;&gt;
			&lt;/filePath&gt;
		&lt;/fileSet&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://YOUR_TARGET_NAMENODE:8020/data&lt;/datastore&gt;
			&lt;path type=&quot;folder&quot; path=&quot;/2013/01&quot;&gt;&lt;/path&gt;
			&lt;trimPrefix&gt;/logs&lt;/trimPrefix&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;FS_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;FS_WRITER&quot;&gt;
		&lt;/writer&gt;
	&lt;/batch&gt;
&lt;/JobSpec&gt;
</pre>
</div></div><p><strong>Sample Pushstream job</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;&gt;
	&lt;name&gt;name&lt;/name&gt;
	&lt;type&gt;streaming&lt;/type&gt;
	&lt;streaming type=&quot;push&quot;&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://YOUR_TARGET_NAMENODE:8020/&lt;/datastore&gt;
			&lt;path&gt;/tmp&lt;/path&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;PUSH_STREAM_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;PUSH_STREAM_WRITER&quot;&gt;
                &lt;!-- rolling.size is 10KB --&gt;
      &lt;property name=&quot;rolling.size&quot;    value=&quot;10240&quot;/&gt;
 		&lt;/writer&gt;
	&lt;/streaming&gt;
&lt;/JobSpec&gt;
</pre>
</div></div><p>This spec is telling Dataloader to create a pushstream job; the received data will be stored to the HDFS datastore <a href="hdfs://YOUR_TARGET_NAMENODE:8020/" rel="nofollow">hdfs://YOUR_TARGET_NAMENODE:8020/</a> to the folder /tmp. The number of workers to start is specified in the transfer policy.</p><p><strong>Sample Pullstream job</strong></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: html/xml; gutter: false" style="font-size:12px;">&lt;JobSpec xmlns=&quot;http://www.pivotal.com/gphd/dataloader&quot;
	xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
	xsi:schemaLocation=&quot;http://www.greenplum.com/gphd/dataloader ../../../main/xsd/JobSpec.xsd &quot;&gt;
	&lt;name&gt;streaming-job-1&lt;/name&gt;
	&lt;type&gt;streaming&lt;/type&gt;
	&lt;streaming type=&quot;pull&quot;&gt;
		&lt;feedSet&gt;
			&lt;datastore&gt;http://YOUR_SOURCE_HTTPSERVER:80/&lt;/datastore&gt;
			&lt;feed path=&quot;/updates/2013/01&quot;&gt;&lt;/feed&gt;
			&lt;feed path=&quot;/updates/2013/02&quot;&gt;&lt;/feed&gt;
		&lt;/feedSet&gt;
		&lt;destination&gt;
			&lt;datastore&gt;hdfs://YOUR_TARGET_NAMENODE:8020/&lt;/datastore&gt;
			&lt;path&gt;/2013&lt;/path&gt;
		&lt;/destination&gt;
		&lt;reader type=&quot;HTTP_PULLSTREAM_READER&quot;&gt;
		&lt;/reader&gt;
		&lt;writer type=&quot;FS_ROLL_WRITER&quot;&gt;
		&lt;/writer&gt;
	&lt;/streaming&gt;
&lt;/JobSpec&gt;
</pre>
</div></div><p>This spec is telling Dataloader to create a pull stream job to get data from the two specified path in the HTTP server and store to the HDFS datastore <a href="hdfs://YOUR_TARGET_NAMENODE:8020/" rel="nofollow">hdfs://YOUR_TARGET_NAMENODE:8020/</a> to the folder /2013.</p>
                    </div>

                    
                                                      
                </div>             </div> 
            <div id="footer" style="background: url(http://confluence.greenplum.com/images/border/border_bottom.gif) repeat-x;">
                <p><small>Document generated by Confluence on Sep 23, 2013 15:58</small></p>
            </div>
        </div>     </body>
</html>
