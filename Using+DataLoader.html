<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    <head>
        <title>Hadoop : Using DataLoader</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body>
        <div id="page">
            <div id="main">
                <div id="main-header" class="pageSectionHeader">
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Hadoop : Using DataLoader
                        </span>
                    </h1>

                    <div class="page-metadata">
                        <p>This page last changed on Sep 05, 2013 by <font color="#0050B2">beckmj</font>.</p>
                    </div>
                </div>

                <div id="content" class="view">
                    <div id="main-content" class="wiki-content group">
                    <h1 id="UsingDataLoader-OverallWorkflow">Overall Workflow</h1><p>You can use the command line tool or Web GUI to perform the tasks listed below (in order of user activities). The Web GUI currently only supports batch jobs; streaming jobs must be started through the CLI. Once started, streaming jobs can be managed through the Web GUI. <br />You must be logged in to have execution access .<br />You can:</p><ul><li>Register sources and destination as data stores</li><li>Create a data-loading job<ul><li>Specify a data store as Source</li><li>Specify a data store (HDFS for GPHD 1.2, HDFS2 for Pivotal HD) as Destination</li><li>Select Chunking / Compressing data during transfer</li><li>Select the Copy Strategy</li><li>Insert any other processing JARs, e.g., encryption</li></ul></li><li>Submit a Job</li><li>Query a job</li><li>List jobs</li><li>Monitor job progress</li></ul><p>By default, an anonymous user has only read-only access to the Web GUI. <br />A user must be logged in to perform job/datastore operations. A user can only suspend/resume/cancel jobs that are submitted by this user.<br />For a user to perform these functions, use the following steps:</p><ol><li>Create a Linux user account on the machine where DataLoader manager is installed.</li><li>Login as the user on the Web GUI or ssh as the user to the command line to get execution access.<span style="line-height: 1.4285715;">Users must have access to the source data.</span></li><li><span style="line-height: 1.4285715;">Create a /home/username directory in the destination HDFS.</span></li></ol><p><span style="line-height: 1.4285715;">To be able to write data to HDFS, either a user home directory must be present in the destination HDFS, or the user must be part of the superuser group in the Hadoop cluster.</span></p><p><span style="line-height: 1.4285715;">By default, dataloader creates a single &quot;dladmin&quot; user for this purpose. If using dladmin, a /home/dladmin directory needs to be created in destination hdfs or dladmin needs to be a superuser. If using dladmin, dladmin needs to have access to source data.</span></p><h1 id="UsingDataLoader-Aboutjoblifecycles">About job lifecycles</h1><h2 id="UsingDataLoader-Batchjoblifecycle">Batch job lifecycle</h2><p>When a batch job is submitted, it is first in SUBMITTED state. Batch jobs automatically start loading data and change their job status to RUNNING. After all the data has successfully loaded to its destination, the job status changes to COMPLETED. If the job fails, the status changes to FAILED.</p><p>A batch job can be SUSPENDED. When suspended, the job retains the list of files that have already been copied. A suspended batch job can be RESUMED. When resumed, the job resumes execution and only copies the files that are left uncopied. When chunking is enabled, only those chunks still to be copied are copied after resuming a suspended job. Loading of chunks or files that were in the middle of transfer when the job was SUSPENDED is restarted.</p><p>A batch job can also be CANCELED. Once canceled, the job is put into CANCELED job state, and cannot be resumed.</p><h2 id="UsingDataLoader-Streamingjoblifecycle">Streaming job lifecycle</h2><p>When a streaming job is submitted, it is first in SUBMITTED state. DataLoader scheduler first determines how many workers are needed to run this streaming job. For push streaming, since the number of clients that will connect to the workers is unknown, you must specify the number of workers (in the job transfer policy) when submitting the job. For pull streaming, scheduler will calculate the required number of workers to run the job, based on the number of feeds you specified. You can also specify the number of workers explicitly.</p><p>Once a job has successfully entered its SUBMITTED state, DataLoader service will start the job right away. The job changes to STARTED state. When the required number of workers are up and running, the job will change to the RUNNING state. In this state, for push stream jobs, the workers are ready to accept client connections; you must stop (cancel) the job explicitly to complete the job. For pull streaming jobs, after all the data has successfully streamed to its destination, the job status changes to COMPLETED. If the job fails, the status changes to FAILED.</p><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">A streaming job can be SUSPENDED. When suspended, all stream workers will be released. A suspended job can be RESUMED. When resumed, the job continues appending to the job storage files. </span></p><p>A job can also be CANCELED. Once canceled, the job is put into a canceled job state, and cannot be resumed.</p><h1 id="UsingDataLoader-Registeringandunregisteringdatastores">Registering and unregistering data stores</h1><h2 id="UsingDataLoader-ManagingdatastoresthroughtheDataLoaderconsole">Managing data stores through the DataLoader console</h2><p>To load data from a data store, the data store must be registered with Pivotal DataLoader. <br />The easiest way to register a new data store is through the DataLoader console web interface.</p><h3 id="UsingDataLoader-RegisteringaDataStore">Registering a Data Store</h3><p>Go to the web page at http://&lt;dataloader_service_hostname&gt;:12380/manager <br />The default port for the Web UI is 12380.This port may change according to your configuration.</p><ol><li>Click on the upper right corner to log in.</li><li>Click on the Data Stores entry on the Task bar to enter the data store instance page.</li><li>Click on Register New Data Store to bring up the New Data Store Instance page.</li><li>On the New Data Store Instance page, enter the type of datastore (HDFS2, LOCALFS, FTP, NFS, or HDFS), host, port, root path, username/password when required, and any additional properties.</li></ol><p>DataLoader maintains the source directory structure from the root directory when copying. Thus, you must set the root directory accordingly: for example, if the source directory is in /home/testuser, set the directory to that destination.<br /><strong>Notes:</strong></p><ul><li>To register a NFS type data store, you should mount the NFS share to both the master machine and Hadoop tasktracker nodes (node-manager nodes for YARN). Also, all NFS nodes must mount to the same directory structure.</li><li>LocalFS can only be used with local disk data on DataLoader worker nodes (Hadoop DataNodes). You can register any DataLoader slave machine in the Hadoop cluster as a localFS type data store.</li><li>When using DataLoader in distributed mode, for LocalFS data stores, you must change the Fairscheduler with DataLoader's modified scheduler. The cluster must be restarted for HDFS 2.x. Refer to the <a href="Data%2BStores.html">“<em>Data Stores</em></a>” page for details.</li></ul><p><br />To register an HDFS datastore, the host is namenode, and the port is typically 8020. <br />If you need to specify more data store properties select <strong>Add Property</strong>. See <a href="Data%2BStores.html">Data Stores</a> for more information.</p><p><br />Click the <strong>Create</strong> button on the page to complete the registration. The new data store will appear on the Data Store Instances page. if you entered invalid information, you will be warned that the datastore cannot be registered.</p><h3 id="UsingDataLoader-SupportedDatastoreTypes"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Supported Datastore Types</span></h3><p><span style="line-height: 1.4285715;">DataLoader supports the following datastores.</span></p><ul><li>nfs</li><li>ftp</li><li>localFS</li><li>GPHD1.2</li><li>Pivotal HD 1.0</li><li>Apache Hadoop 1.0.3</li><li>Apache Hadoop 2.0.2-alpha</li></ul><p>Regarding datastore registration properties,refer to the “<a href="Data%2BStores.html">Data Stores</a>” section for more information.</p><h2 id="UsingDataLoader-Un-registeringorDeletingaDataStore">Un-registering or Deleting a Data Store</h2><ol><li>Log in, if not already logged in.</li><li>Click on the Data Stores tab to see the Data Store Instances page.</li><li>Click the Remove button of the data store you want to remove.</li></ol><h2 id="UsingDataLoader-Managingadatastorethroughthecommandline"><span style="line-height: 1.5;">Managing a data store through the command line</span></h2><ol><li>To register a data store using the command line</li><li>Perform the following command line operations on the DataLoader CLI machine.</li><li>On the Client node, create the property file for the datastore as &lt;propertyFile&gt;.</li></ol><p>DataLoader maintains the source directory structure from the root directory when copying. Thus, you must set the root directory accordingly: i.e., if the source directory is in /home/testuser, set the directory to that destination.<br />The following is an example of a property file.</p><pre>type=localfs<br />host=&lt;hostname&gt;<br />rootpath=/<br />username=test_user<br />password=test_user</pre><p>Datastore sample property files are not included in the RPM package.<br />For information regarding datastore registration properties, refer to the section on “Data Stores” for more information.</p><p><span style="line-height: 1.4285715;">Run the following command to register the data store in CLI node.</span></p><pre>$ cd /usr/local/gphd/dataloader-2.0.1/bin/ <br />$ ./dataloader-cli.sh config --command register -i &lt;propertyfile&gt;</pre><p><span style="line-height: 1.4285715;">Create a &lt;propertyfile&gt; for the datastore as shown in “<a href="Data%2BStores.html">Data Stores</a>”. For example, to register an nfs datastore, the commands would be:</span></p><pre>$ cd /usr/local/gphd/dataloader-2.0.1/bin <br />$ ./dataloader-cli.sh config --command register -i nfs.property</pre><p>The host must be the hostname of the node.</p><h2 id="UsingDataLoader-Un-registeringorDeletingaDataStorethroughtheCommandLine">Un-registering or Deleting a Data Store through the Command Line</h2><p>Run the following command to list all the registered datastore IDs.</p><pre>$ cd /usr/local/gphd/dataloader-2.0.1/bin <br />$ ./dataloader-cli.sh config --command list</pre><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">Run the following command to delete the datastore.</span></p><pre>$ ./dataloader-cli.sh config --command delete -n datastore_ID</pre><p><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 24.0px;line-height: 1.25;">Managing Jobs through DataLoader Web Console</span></p><p>Batch jobs can be managed through DataLoader Web Console.</p><p>Open a browser window with the URL:<br />http://&lt;dataloader_service_hostname&gt;:12380/manager/<br />The default port for the Web UI is 12380.</p><h2 id="UsingDataLoader-CreatingandsubmittingaJob"><span style="line-height: 1.5;">Creating and submitting a Job</span></h2><p><span style="line-height: 1.4285715;">Currently, only batch jobs can be created and submitted this way. Stream jobs must be submitted through the CLI.</span></p><ol><li>Click on the upper right corner to log in.</li><li> Select Create a New Job on the <strong>Home</strong> page.</li><li>You can submit your job using either basic or advanced properties.</li></ol><p style="margin-left: 30.0px;">To submit using the basic option, provide the required values shown in Basic property values for submitting a job. You will need to specify the type of source datastore. The available datastores will appear in the Source Path. Click on the green button to add a source datastore. It will appear in the <strong>Selected</strong> field.</p><p style="margin-left: 30.0px;">The available targets are shown in the <strong>Target Path</strong> field. Click to select a target.<br />The default copy strategy is Intelligent, which selects the best strategy, based on the data being loaded. For more information, see .<a href="DataLoader%2BCopy%2BStrategy%2Band%2BTransfer%2BPolicy.html">DataLoader Copy Strategy and Transfer Policy</a>.</p><p style="margin-left: 30.0px;"><span style="line-height: 1.4285715;">The Basic properties for submitting a job are as follows:</span></p><p><span style="line-height: 1.4285715;"> </span></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd">Source Datastore</td><td class="confluenceTd">The source dta store hostname and port</td></tr><tr><td class="confluenceTd">Source Path</td><td class="confluenceTd">The path of the data you want to load into the source data store</td></tr><tr><td class="confluenceTd">Target URI</td><td class="confluenceTd">The destination data store hostname and port.</td></tr><tr><td class="confluenceTd">Target Path</td><td class="confluenceTd">The path of the data you want to copy to in the destination data store.</td></tr></tbody></table></div><p style="margin-left: 30.0px;"><span style="line-height: 1.4285715;">If you wish to submit a job using Advanced options, click on <strong>Show Advanced Options</strong>.</span></p><p style="margin-left: 30.0px;"><span style="line-height: 1.4285715;">The table below shows the Advanced Options. For more information on the Advanced Options, refer to the Job Transfer Policy in the  section <a href="DataLoader%2BCopy%2BStrategy%2Band%2BTransfer%2BPolicy.html">DataLoader Copy Strategy and Transfer Policy</a>.</span></p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Description</th></tr><tr><td class="confluenceTd">Worker Number</td><td class="confluenceTd">Number of workers to start for this job</td></tr><tr><td class="confluenceTd">Bandwidth Throttling</td><td class="confluenceTd">Enable throttling at the specified Bandwidth setting.</td></tr><tr><td class="confluenceTd">Bandwidth</td><td class="confluenceTd">Maximum bandwidth used before throttling is enabled.</td></tr><tr><td class="confluenceTd">Chunking</td><td class="confluenceTd"><p>Use chunking when conditions in Chunking Threshold and Chunking size are met. Cannot be<br />used if Compression is enabled.</p></td></tr><tr><td class="confluenceTd">Chunking Threshold</td><td class="confluenceTd"><p>96M by default</p><p> </p></td></tr><tr><td class="confluenceTd">Chunking Size</td><td class="confluenceTd"><span>64M by default</span></td></tr><tr><td colspan="1" class="confluenceTd">Overwrite existing file</td><td colspan="1" class="confluenceTd">Default is to not overwrite</td></tr><tr><td colspan="1" class="confluenceTd">Compression</td><td colspan="1" class="confluenceTd">Compress payload for transfer using Snappy. Cannot be used if Chunking is enabled.</td></tr><tr><td colspan="1" class="confluenceTd">Workers per Disk</td><td colspan="1" class="confluenceTd">The number of workers that will copy the files in a single disk.</td></tr></tbody></table></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">4. Select </span><strong style="line-height: 1.4285715;">Submit</strong><span style="line-height: 1.4285715;">.</span></p><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">Greenplum Data Loader uses the default value for optional fields. After submitting the job, you can search under the <strong>Running Job</strong>s list in the home page.</span></p><p><span style="line-height: 1.4285715;"> 5</span><span style="line-height: 1.4285715;">. Check the detailed information about the job by clicking the job ID.</span></p><h3 id="UsingDataLoader-MonitoringaJob"><strong style="color: rgb(0,0,0);line-height: 1.4285715;">Monitoring a Job</strong></h3><p><span style="line-height: 1.4285715;">Click the job ID to monitor details and find detailed job information.</span></p><p>You can check the progress bar (for batch jobs) or see the active workers (for streaming jobs).<br /><strong>Note:</strong> If files do not refresh, clear the browser cache</p><h3 id="UsingDataLoader-SuspendingaJob"><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Suspending a Job</span></h3><ol><li><span style="line-height: 1.4285715;">In the Running Job list on the home page, find the job ID.</span></li><li>Select <strong>Suspend</strong>.</li><li>The suspended job can be found in the suspended job list.</li></ol><h3 id="UsingDataLoader-ResumeaJob"><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Resume a Job</span></h3><ol><li><span style="line-height: 1.4285715;">Click on the upper right corner to log in.</span></li><li>Click <strong>Home</strong> to go to the Home page.</li><li>In the Running Job list on the Home page, find the job ID .</li><li>Select <strong>Resume</strong> in the Job Operations list. DataLoader will warn you that the Resume command has been sent and will change to RUNNING or STARTED state.</li><li>You can check the home page to confirm that the job is running again.</li></ol><h3 id="UsingDataLoader-StoppingaJob"><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Stopping a Job</span></h3><ol><li><span style="line-height: 1.4285715;">Click on the upper right corner to log in.</span></li><li>Click <strong>Home</strong> to go to the Home page.</li><li>On the Home page, search the Running Jobs or the Suspended Jobs list to find the job you want to stop.</li><li>Select the Job ID to go to the Job Detail page.</li><li>From the Job Operation list, select the Cancel button. DataLoader will warn you that the Cancel command has been sent and will change to CANCELLED state.</li><li>Confirm that the job is listed in the Canceled Jobs list.</li></ol><p><span style="color: rgb(0,0,0);font-size: 20.0px;line-height: 1.5;">Managing Jobs Through the Command Line</span></p><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">Through the command line, you can manage both batch jobs and streaming jobs.</span></p><h2 id="UsingDataLoader-SubmittingandCreatingaJobThroughtheCommandLine"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Submitting and Creating a Job Through the Command Line</span></h2><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">1. </span><span style="line-height: 1.4285715;">Prepare the Job Specification File</span></p><p>You must prepare the transfer specification file to submit a job using the command line. The transfer specification file is an XML file that specifies the source and destination data store, and the file or folder to load. Refer to the section “<em><a href="DataLoader%2BJob%2BSpecification.html">Job Specification</a></em>” for more information and job spec samples.</p><p><span style="line-height: 1.4285715;"> </span></p><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">2. Submit job through command line after the transfer specification file is ready.</span></p><p><span style="line-height: 1.4285715;"> Some options in the Policy file can also be specified as a command argument. If this is the case, the Command Line argument will take precedence. </span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh submit -i &lt;job specification file&gt; -s &lt;strategy&gt;
</pre>
</div></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">See the <a href="DataLoader%2BCommand%2BLine%2BInterface.html"><em>DataLoader</em> <em>Command Line Reference</em></a> for more information.</span></p><p><span style="line-height: 1.4285715;"> </span><strong style="line-height: 1.4285715;">Note</strong><span style="line-height: 1.4285715;">: If user submit job via command line, Dataloader will use the current user to copy data to the destination path, so please make sure the target path is writable to the current user.</span></p><h3 id="UsingDataLoader-Listjobswithaspecificstate"><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">List jobs with a specific state</span></h3><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">List all jobs in a specified state, for example:</span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh list --status RUNNING</pre>
</div></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">The above command lists all jobs in RUNNING state. You can also get jobs with other states.</span></p><h3 id="UsingDataLoader-MonitoringaJobUsingCommandLine"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Monitoring a Job Using Command Line</span></h3><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">Check the job status and progress through the query command line option;</span></p><p> </p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh query -n &lt;jobid&gt;
</pre>
</div></div><h3 id="UsingDataLoader-SuspendingaJobUsingCommandLine"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">Suspending a Job Using Command Line</span></h3><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh suspend -n &lt;jobid&gt;
</pre>
</div></div><h3 id="UsingDataLoader-ToResumeaJobUsingCommandLine"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">To Resume a Job Using Command Line</span></h3><p><span style="line-height: 1.4285715;"> </span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh resume -n &lt;jobid&gt;
</pre>
</div></div><h3 id="UsingDataLoader-ToStopaJobUsingCommandLine"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">To Stop a Job Using Command Line</span></h3><p><span style="line-height: 1.4285715;"> </span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh stop -n &lt;jobid&gt;
</pre>
</div></div><h1 id="UsingDataLoader-ToPushdatatoDataloaderStreamingjobs"><span style="color: rgb(0,0,0);font-size: 20.0px;line-height: 1.5;">To Push data to Dataloader Streaming jobs</span></h1><p>You must first create a push stream job as explained in last section. After the job is started, the DataLoader CLI can be used to push data to the push stream job. The sections below show the basic usage of push data. An end-to-end example is given in the section following. For more details about the CLI options, please refer to the “DataLoader Command Line Reference”.</p><h3 id="UsingDataLoader-Topushdatafromsinglefilethatdoesnotchange:"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">To push data from single file that does not change:</span></h3><p><span style="line-height: 1.4285715;"> </span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh pcat -n &lt;jobid&gt; --source testdata.log --format TEXT</pre>
</div></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">This command will read data from </span><em style="line-height: 1.4285715;">testdata.log</em><span style="line-height: 1.4285715;"> line-by-line and push them to the specified dataloader streaming job. CLI will exit when the end of this file is reached.</span></p><h3 id="UsingDataLoader-Topushdatafromsinglefilethatkeepschanging"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">To push data from single file that keeps changing</span></h3><p> </p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh ptail -n &lt;jobid&gt; --source testdata.log --format TEXT --idle 10000</pre>
</div></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">This command will read data from </span><em style="line-height: 1.4285715;">testdata.log</em><span style="line-height: 1.4285715;">, line-by-line, and push these to the specified dataloader streaming job. CLI will exit only if this testdata.log has not been changed for 10 seconds. If the --idle option is not set, CLI will keep polling the changes in this file until error is detected or the job is closed.</span></p><h3 id="UsingDataLoader-TopushdatafromSTDIN"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">To push data from STDIN</span></h3><p><span style="line-height: 1.4285715;"> </span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh pstream -n &lt;jobid&gt;</pre>
</div></div><p><span style="line-height: 1.4285715;">This command will read data from STDIN (line-by-line) and push them to the specified dataloader streaming job. CLi will exit when it receives an end-of-stream signal (ctrl+d).</span></p><p>One possible use case might be:</p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ cat testdata.log | dataloader-cli.sh pstream -n &lt;jobid&gt;</pre>
</div></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">This will push the content of </span><em style="line-height: 1.4285715;">testdata.log</em><span style="line-height: 1.4285715;"> to the specified job.</span></p><h3 id="UsingDataLoader-Topushdatafromaspecifiedfolder"><span style="line-height: 1.4285715;"> </span><span style="color: rgb(0,0,0);font-size: 16.0px;line-height: 1.5625;">To push data from a specified folder</span></h3><p><span style="line-height: 1.4285715;"> </span></p><div class="code panel pdl" style="border-width: 1px;"><div class="codeContent panelContent pdl">
<pre class="theme: Confluence; brush: java; gutter: false" style="font-size:12px;">$ dataloader-cli.sh scan -n &lt;jobid&gt; --scanfolder /data/testdata/ --thread-pool-size 4</pre>
</div></div><p><span style="line-height: 1.4285715;"> </span><span style="line-height: 1.4285715;">This command scans the folder /data/testdata for files and pushes them to the specified dataloader streaming job. CLI will start four threads to read the data concurrently. CLI will keeping scanning this folder for new files until it is explicitly stopped by a Ctrl+c. command.</span></p><p>The thread pool size setting determines the number of threads to startto read files from a specifiedfolder. Each thread will read data from a single file until all the data in the file is transferred to the destination, then the thread will move to next file in the folder.<br />Increasing the number of threads can potentially enhance parallel processing of I/O, provided that the destination worker has sufficient capacity.</p><h3 id="UsingDataLoader-PushstreamExample">Pushstream Example</h3><p>In this example, the job is run from the command line. <br />Log into the DataLoader CLI node.<br />Create the file “file1” using below command</p><pre># dd if=/dev/zero of=/file1 bs=1024 count=1024</pre><p>Register the HDFS datastore:</p><p><br />Go to the web page at <a href="http://localhost:12380/manager" class="external-link" rel="nofollow">http://localhost:12380/manager</a> and select the Data Stores button to enter the data store register page.<br />Select the Register New Data Store button to bring up the New Data Store Instance page.</p><p>Enter values in the New Data Store Instance page to register new data store.</p><p>type=hdfs<br />host=&lt;hdfs namenode host&gt;<br />port=&lt;hdfs port&gt;<br />rootpath=/</p><p>Click the <strong>Create</strong> button on the page to complete the registration. The new data store can be found in the Data Store Instances page.<br />Run the job via the command line by performing the following steps:</p><ol><li>Log into the DataLoader CLI node.</li><li><span style="line-height: 1.4285715;">Prepare job specification “spec.xml” under /usr/local/gphd/dataloader-2.0.1/bin as follows:</span></li></ol><p><br />&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;yes&quot;?&gt;<br />&lt;JobSpec xmlns=&quot;<a href="http://www.pivotal.com/hd/dataloader" class="external-link" rel="nofollow">http://www.pivotal.com/hd/dataloader</a>&quot;&gt;<br />&lt;name&gt;name&lt;/name&gt;<br />&lt;type&gt;streaming&lt;/type&gt;<br />&lt;streaming type=&quot;push&quot;&gt;<br />&lt;destination&gt;<br />&lt;datastore&gt;<a rel="nofollow">hdfs2://Your_TARGET_NAMENODE:8020/</a>&lt;/datastore&gt;<br />&lt;path&gt;/2013&lt;/path&gt;<br />&lt;/destination&gt;<br />&lt;reader type=&quot;PUSH_STREAM_READER&quot;&gt;<br />&lt;/reader&gt;<br />&lt;writer type=&quot;PUSH_STREAM_WRITER&quot;&gt;<br />&lt;!-- rolling.size is 10KB --&gt;<br />&lt;property name=&quot;rolling.size&quot; value=&quot;10480&quot;/&gt;<br />&lt;/writer&gt;<br />&lt;/streaming&gt;<br />&lt;/JobSpec&gt;</p><p style="margin-left: 30.0px;">3. Prepare a policy file “policy.property” under /usr/local/gphd/dataloader-2.0.1/bin.</p><pre>WORKER_NUMBER=&lt;n&gt;<br />IS_BAND_WIDTH_THROTTLING=false<br />BAND_WIDTH=0<br />USE_COMPRESSION=false<br />STRATEGY=pushstream</pre><p style="margin-left: 30.0px;">4. Specify the worker number to the number of DataLoader workers available in the cluster.<br />Execute following command to submit the job:</p><pre># cd /usr/local/gphd/dataloader-2.0.1/bin<br /># ./dataloader-cli.sh submit -i spec.xml -s pushstream –p policy.property</pre><p>The job is submitted and jobID is displayed on the console. Please note the JobID.</p><p>Push file to the job.</p><pre># cd /usr/local/gphd/dataloader-2.0.1/bin<br /># cat /file1 | sh dataloader-cli.sh pstream -n &lt;JobID&gt;</pre><p>Once the file gets uploaded, the DataLoader job can be cancelled:</p><pre># cd /usr/local/gphd/dataloader-2.0.1/bin<br /># ./dataloader-cli.sh cancel –n &lt;jodID&gt;</pre><p>Verify that the files were transferred to the destination FS.</p><h1 id="UsingDataLoader-Troubleshooting">Troubleshooting</h1><p>Check the following DataLoader log files under directory: /var/log/gphd/dataloader/</p><ul><li>dataloader-manager.log</li><li>dataloader-manager.out</li><li>dataloader-scheduler.log</li><li>dataloader-scheduler.out</li></ul><p>The DataLoader log file for the command line interface is located in the directory where you run the commands: dataloader-cli.log</p><p>It is very helpful to check the Hadoop cluster log to track the status of the DataLoader workers. Check the Hadoop cluster log files under: /user/lib/gphd/hadoop/logs/</p><p>Check the Hadoop NameNode monitoring page:<br /><a href="http://namenode.host:50070/dfshealth.jsp" class="external-link" rel="nofollow">http://namenode.host:50070/dfshealth.jsp</a><br />The YARN cluster resource monitoring is located at:<br /><a href="http://resourcemanager.host:8088/cluster" class="external-link" rel="nofollow">http://resourcemanager.host:8088/cluster</a></p><p><span style="line-height: 1.4285715;"> </span></p><p> </p><p><span style="line-height: 1.4285715;"> </span></p><p><span style="line-height: 1.4285715;"><br /></span></p>
                    </div>

                    
                                                      
                </div>             </div> 
            <div id="footer" style="background: url(http://confluence.greenplum.com/images/border/border_bottom.gif) repeat-x;">
                <p><small>Document generated by Confluence on Sep 23, 2013 15:58</small></p>
            </div>
        </div>     </body>
</html>
