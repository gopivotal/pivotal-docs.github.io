<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    <head>
        <title>Hadoop : Loading Files and Push Streams into HAWQ Using PXF</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body>
        <div id="page">
            <div id="main">
                <div id="main-header" class="pageSectionHeader">
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Hadoop : Loading Files and Push Streams into HAWQ Using PXF
                        </span>
                    </h1>

                    <div class="page-metadata">
                        <p>This page last changed on Aug 12, 2013 by <font color="#0050B2">beckmj</font>.</p>
                    </div>
                </div>

                <div id="content" class="view">
                    <div id="main-content" class="wiki-content group">
                    <p>DataLoader writes push streams and text/Avro files into HDFS, using push stream clients. The data is written in text or Apache Avro format, which is exposed through DataLoader APIs. You can either run batch jobs like MapReduce to analyze the data, or utilize Pivotal HAWQ to perform fast SQL queries on the data. <br />This document describes how to configure HAWQ/PXF to import data that is written to HDFS by DataLoader.</p><h1 id="LoadingFilesandPushStreamsintoHAWQUsingPXF-Prerequisites">Prerequisites</h1><p>To perform this operation, you need a running HAWQ and PXF on top of Pivotal HD.<br />HAWQ is a powerful SQL engine running on top of Pivotal HD; PXF is a connector between HAWQ and Pivotal HD, which provides HAWQ the ability to read files on HDFS and perform SQL queries. For more details on setting up HAWQ and PXF, please refer to their related documentation.</p><h1 id="LoadingFilesandPushStreamsintoHAWQUsingPXF-ConfiguretheDataLoaderPXFplugin">Configure the DataLoader PXF plugin</h1><p>To analyze stream data with HAWQ, you must first configure a PXF plugin so that PXF understands how to read DataLoader's proprietary format in Avro and translate it into database tuples, which can then be directly accessed by HAWQ.</p><p>Download the dataloader-gpxf-plugin-2.0.1-bin.tar.gz from the web site. If you have dataloader-service already installed, you can find it in /usr/local/gphd/dataloader-2.0.1/plugins/gpxf.</p><p>Untar dataloader-gpxf-plugin-&lt;version&gt;.tar.gz and put the extracted jar files in PXF's lib directory, which also contains PXF's jar files such as gpxf-&lt;version&gt;.jar or pxf-&lt;version&gt;.jar. Currently, there are only two jar files in the tar ball:</p><ul><li>dataloader-gpxf-plugin-&lt;version&gt;.jar</li><li>dataloader-common-&lt;version&gt;.jar.</li></ul><p>Add these two jars into Hadoop's classpath by modifying hadoop-env.sh and adding them to the list in $HADOOP_CLASSPATH, as PXF does.</p><p>Restart HDFS to implement the change.<br />You can now test HAWQ on DataLoader’s stream data.</p><h1 id="LoadingFilesandPushStreamsintoHAWQUsingPXF-End-to-EndUseCase">End-to-End Use Case</h1><p>Currently, DataLoader's push stream CLI support two formats: TEXT and Avro. You can push either common text files or Avro data files to HDFS in a dataloader job.</p><p>To load the data into HAWQ, the data for a particular push stream job must have the same data schema across source clients connected to the job. You must create separate push stream jobs for streams with different schema.</p><h2 id="LoadingFilesandPushStreamsintoHAWQUsingPXF-LoadingTEXTFormatintoHAWQ">Loading TEXT Format into HAWQ</h2><p>For TEXT format, the file can be a CSV format where each line represents a record and each record contains fields separated by delimiters such as commas, for example:<br />John,male,18<br />Mary,female,30<br />David,male,70</p><p>The following example will use this sample text file.</p><ol><li>First, push data from the text file, using the DataLoader CLI. In his example, we will use pcat.</li></ol><p style="margin-left: 30.0px;">$ dataloader-cli.sh pcat -n &lt;job-id&gt; --source sample_text_file --format TEXT</p><p style="margin-left: 30.0px;">Invoke the HAWQ console with psql and create an external table, using the destination folder specified for the push stream job when the job is initiated.</p><p style="margin-left: 30.0px;">gpadmin=# create external table csv_table (username text, gender text, age int) location('<a href="pxf://namenode:50070/job/target/path/Job-Id?FRAGMENTER=HdfsDataFragmenter&amp;ACCESSOR=DataLoaderAvroFileAccessor&amp;RESOLVER=TextResolver" rel="nofollow">pxf://namenode:50070/job/target/path/Job-Id?FRAGMENTER=HdfsDataFragmenter&amp;ACCESSOR=DataLoaderAvroFileAccessor&amp;RESOLVER=TextResolver</a>') FORMAT 'TEXT' (DELIMITER=',');</p><p>The table is created with the following database schema:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Field Name</th><th class="confluenceTh">Data Type</th></tr><tr><td class="confluenceTd">username</td><td class="confluenceTd">text</td></tr><tr><td class="confluenceTd">gender</td><td class="confluenceTd">text</td></tr><tr><td class="confluenceTd">age</td><td class="confluenceTd">int</td></tr></tbody></table></div><p>The format is set as &quot;TEXT&quot; using commas as the delimiters, matching the data schema of the sample text files.</p><p>You can now make SQL queries using the HAWQ console.</p><p style="margin-left: 30.0px;">gpadmin=# select username from csv_table</p><h2 id="LoadingFilesandPushStreamsintoHAWQUsingPXF-LoadingAvroDataintoHAWQ">Loading Avro Data into HAWQ</h2><p>Avro format requires that all Avro format files have the same Avro schema. For example:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh"><p>{<br /> &quot;type&quot; : &quot;record&quot;,<br /> &quot;name&quot; : &quot;Person&quot;,<br /> &quot;doc&quot; : &quot;A sample Person record which contains fields like name, gender and age.&quot;<br /> &quot;fields&quot; : [<br /> {<br /> &quot;name&quot; : &quot;name&quot;,<br /> &quot;type&quot; : &quot;string&quot;,<br /> &quot;doc&quot; : &quot;Name of the person&quot;<br /> },<br /> {<br /> &quot;name&quot; : &quot;gender&quot;,<br /> &quot;type&quot; : &quot;string&quot;,<br /> &quot;doc&quot; : &quot;Gender of the person&quot;<br /> },<br /> {<br /> &quot;name&quot; : &quot;age&quot;,<br /> &quot;type&quot; : &quot;int&quot;,<br /> &quot;doc&quot; : &quot;Age of the person&quot;<br /> }<br /> ]<br />}</p></th></tr></tbody></table></div><p> </p><p>Suppose you create an Avro file named sample_avro_file using the above Avro schema, the following steps describe how to push the file into HAWQ:</p><p>First, push data from the Avro file using the DataLoader CLI. In this example, we will use pcat.</p><p style="margin-left: 30.0px;">$ dataloader-cli.sh pcat -n &lt;job-id&gt; --source sample_avro_file --format AVRO</p><p>Invoke the HAWQ console with psql and create an external table, using the destination folder specified for the push stream job when the job is initiated.</p><p style="margin-left: 30.0px;">gpadmin=# create external table avro_table (username text, gender text, age int) location('<a href="pxf://namenode:50070/target/path/Job-Id?FRAGMENTER=HdfsDataFragmenter&amp;ACCESSOR=DataLoaderAvroFileAccessor&amp;RESOLVER=AvroResolver" rel="nofollow">pxf://namenode:50070/target/path/Job-Id?FRAGMENTER=HdfsDataFragmenter&amp;ACCESSOR=DataLoaderAvroFileAccessor&amp;RESOLVER=AvroResolver</a>') FORMAT 'custom' (formatter='pxfwritable_import');</p><p>The table is created with the following database schema, matching the Avro schema in the sample file:</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Field Name</th><th class="confluenceTh">Data Type</th></tr><tr><td class="confluenceTd">username</td><td class="confluenceTd">text</td></tr><tr><td class="confluenceTd">gender</td><td class="confluenceTd">text</td></tr><tr><td class="confluenceTd">age</td><td class="confluenceTd">int</td></tr></tbody></table></div><p>You can now make SQL queries using the HAWQ console.</p><p style="margin-left: 30.0px;">gpadmin=# select username from avro_table;</p>
                    </div>

                    
                                                      
                </div>             </div> 
            <div id="footer" style="background: url(http://confluence.greenplum.com/images/border/border_bottom.gif) repeat-x;">
                <p><small>Document generated by Confluence on Sep 23, 2013 15:58</small></p>
            </div>
        </div>     </body>
</html>
