<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
    <head>
        <title>Hadoop : Data Stores</title>
        <link rel="stylesheet" href="styles/site.css" type="text/css" />
        <META http-equiv="Content-Type" content="text/html; charset=UTF-8">
    </head>

    <body>
        <div id="page">
            <div id="main">
                <div id="main-header" class="pageSectionHeader">
                    <h1 id="title-heading" class="pagetitle">
                                                <span id="title-text">
                            Hadoop : Data Stores
                        </span>
                    </h1>

                    <div class="page-metadata">
                        <p>This page last changed on Aug 14, 2013 by <font color="#0050B2">beckmj</font>.</p>
                    </div>
                </div>

                <div id="content" class="view">
                    <div id="main-content" class="wiki-content group">
                    <p>A data store is a abstraction for a persistent store that could be used to store data.. This appendix describes the properties used to register each supported data store.</p><h1 id="DataStores-SourceandDestinationDatastores">Source and Destination Datastores</h1><p>DataLoader has requirements for how the source data store is configured. These are detailed in the sections below. <br />Currently, only HDFS/HDFS2 is suported as a target datastore. Configuration is the same as for the source datastores.</p><h1 id="DataStores-FTPDataStore">FTP Data Store</h1><p>The table below shows the properties of each type of FTP data store.</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Value</th></tr><tr><td class="confluenceTd">type</td><td class="confluenceTd">ftp</td></tr><tr><td class="confluenceTd">host</td><td class="confluenceTd">The name or IP address of the FTP server.</td></tr><tr><td class="confluenceTd">rootpath</td><td class="confluenceTd">The root path of the source FTP server.</td></tr><tr><td class="confluenceTd">port</td><td class="confluenceTd">The port of the FTP server. The default port is 21.</td></tr><tr><td class="confluenceTd">user</td><td class="confluenceTd">The FTP username. Set as anonymous, if no authentication is needed</td></tr><tr><td class="confluenceTd">passwd</td><td class="confluenceTd">The FTP user’s password.Set as password, if no password is needed.</td></tr><tr><td class="confluenceTd">transfermode</td><td class="confluenceTd">The data transfer mode for this FTP server. Values can be “stream,” “block,” or “compressed.” Default is “stream.”</td></tr><tr><td class="confluenceTd">passive</td><td class="confluenceTd">When the FTP server is configured as passive, this value should be set to true.</td></tr><tr><td class="confluenceTd">timeout</td><td class="confluenceTd">The timeout value used to connect to the FTP server.</td></tr><tr><td colspan="1" class="confluenceTd">filetype</td><td colspan="1" class="confluenceTd">The type of the files on this FTP server. Can be “binary” or “ascii”. Default is binary.</td></tr></tbody></table></div><p>If using the Web GUI, use the following values for the FTP data stores:<br />Type: ftp<br />Host: &lt;hostname&gt;<br />Port: 21<br />Rootpath: /<br />User: ftpuser<br />Password: ftpuser</p><h1 id="DataStores-LocalFSDataStore">LocalFS Data Store</h1><p>Using LocalFS as the source datastore has restrictions:</p><ul><li>Data must be put in a directory that is universally readable, such as /data or /tmp, so that the mapreduce user can access it to copy the data.</li><li>LocalFS needs to be on local disk of a DataLoader worker (Hadoop Datanode) for distributed mode, on the manager node for standalone and pseudo-distributed mode.</li><li>To register LocalFS datastores, the slave machines must be used. You can register any DataLoader worker machines as LocalFS type data stores.</li><li>When using DataLoader in distributed mode, for LocalFS data stores, you must change the Fairscheduler with DataLoader's modified scheduler. The cluster must be restarted for HDFS 2.x.</li><li>The LocalFS host must be within the mrv1 or mrv2 cluster.</li></ul><p>Refer to the table below for the Register Property values.</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Value</th></tr><tr><td class="confluenceTd">type</td><td class="confluenceTd">localfs</td></tr><tr><td class="confluenceTd">host</td><td class="confluenceTd">The name of a local host machine. Data is copied from this machine when you select the localfs strategy.</td></tr><tr><td class="confluenceTd">rootPath</td><td class="confluenceTd">The base path of the file system.</td></tr><tr><td class="confluenceTd">Username</td><td class="confluenceTd">User who has access to the LocalFS.</td></tr><tr><td class="confluenceTd">password</td><td class="confluenceTd">User's Linux password.</td></tr><tr><td class="confluenceTd">port</td><td class="confluenceTd">Port used for the ssh connection. Default is 22.</td></tr></tbody></table></div><p>If using the Web GUI, use the following values for the LocalFS data stores:</p><p>Type: localfs<br />Host: &lt;hostname&gt;<br />Rootpath: /<br />User: test_user<br />Password: test_user</p><p><br />If using the CLI, use the following entries in the Property file:</p><p>type=localfs<br />host=&lt;hostname&gt;<br />rootpath=/<br />username=test_user<br />password=test_user</p><h1 id="DataStores-ConfiguringHadoopforLoadingDatafromLocalFilesystem">Configuring Hadoop for Loading Data from Local Filesystem</h1><p><br />To copy data from the Local Filesystem, you will need to configure your Hadoop cluster to use our specialized FairScheduler.</p><h2 id="DataStores-FairSchedulersetup">FairScheduler setup</h2><p>If you want to transfer data from the local file system, use the localfs copy strategy. With localfs copy strategy, DataLoader assigns the copy task to the node that holds the data, allowing the copy task to read data directly from the local filesystem. FairScheduler has been modified to enforce copy tasks being assigned locally for localfs copy. Set up the FairScheduler according to the following procedure.</p><h3 id="DataStores-YARN/MRV2:">YARN/MRV2:</h3><p>If you are using YARN/MRv2 Hadoop cluster, copy the dataloader-fairscheduler-mrv2*.jar file to $YARN_HOME/lib, e.g. /usr/lib/gphd/hadoop-yarn/lib on the yarn resource manager. You can find this file in /usr/lib/gphd/dataloader-{version}/plugins/fairscheduler.</p><p>Configure YARN to use dataloader-fairscheduler as Resource Manager's scheduler. You can do this by setting a property in yarn-site.xml on the resource manager.</p><p>&lt;property&gt;<br /> &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;<br />&lt;value&gt;com.pivotal.hd.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;<br />&lt;/property&gt;</p><p>If the Resource Manager is running, restart it for the changes to take effect.</p><h3 id="DataStores-MRV1:">MRV1:</h3><p>If you are using the MRv1 Hadoop cluster, replace the Hadoop Fairscheduler jar file under $HADOOP_HOME/lib/hadoop-fairscheduler*.jar with dataloader-fairscheduler-mrv1*.jar.</p><p>Configure JobTracker to use dataloader-fairscheduler by setting a property in mapred-site.xml as shown below.</p><p>&lt;property&gt;<br /> &lt;name&gt;mapred.jobtracker.taskScheduler&lt;/name&gt;<br /> &lt;value&gt;org.apache.hadoop.mapred.FairScheduler&lt;/value&gt;<br />&lt;/property&gt;</p><p>If JobTracker is running, restart it for the changes to take effect. You can now load data from the localfs.</p><h3 id="DataStores-ImpacttoHadoopenvironment">Impact to Hadoop environment</h3><p>Setting FairScheduler to the specialized version can have an impact on the Hadoop environment. By setting up use of the specialized version of FairScheduler for DataLoader, the Hadoop cluster still works as if you set it up to use the original FairScheduler.</p><h1 id="DataStores-NFSDataStore">NFS Data Store</h1><p>Using NFS as a source datastore has the following restriction: <br />NFS share must be mounted on all the nodes, including the master and workers with the same directory structure. <br />Refer to the table  below for NFS Data Store Registration Properties</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Value</th></tr><tr><td class="confluenceTd">type</td><td class="confluenceTd">nfs</td></tr><tr><td class="confluenceTd">host</td><td class="confluenceTd">The host of the NFS share.</td></tr><tr><td class="confluenceTd">rootPath</td><td class="confluenceTd">The starting path relative to the NFS mount point.</td></tr><tr><td class="confluenceTd">port</td><td class="confluenceTd">Port used for the ssh connection. Default is 22.</td></tr></tbody></table></div><p>If using the Web GUI, use the following values for the <span style="line-height: 1.4285715;">NFS data stores:</span></p><p>Type: nfs<br />Host: &lt;hdfs namenode host&gt;<br />Rootpath: /<br />Mountpoint: /mnt/nfs</p><p>If using the CLI, use the following entries in the Property file:<br />type=nfs<br />host=&lt;hostname&gt;<br />rootPath=&lt;rootpath&gt;<br />mountPoint=&lt;mount_point&gt;</p><p>The nfs rootPath represents the share path of the current nfs share.</p><h1 id="DataStores-HDFSDataStore">HDFS Data Store</h1><p>HDFS datastore is able to connect to HDFS that comply with Apache Hadoop 1.x.<br />HDFS authentication should be disabled if using it as a source datastore.</p><p>The table below shows the properties of each type of HDFS data store.</p><p>Properties</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Value</th></tr><tr><td class="confluenceTd">type</td><td class="confluenceTd">hdfs</td></tr><tr><td class="confluenceTd">host</td><td class="confluenceTd"><p>The namenode server of the HDFS cluster.</p></td></tr><tr><td class="confluenceTd">port</td><td class="confluenceTd"><p>The namenode portof the HDFS cluster.</p>.</td></tr><tr><td class="confluenceTd">rootPath</td><td class="confluenceTd">The starting/base of the HDFS namespace..</td></tr></tbody></table></div><p>If using the Web GUI, use the following values for the HDFS Datastore.</p><p><br />Type: hdfs<br />Host: &lt;hdfs namenode host&gt;<br />Port: &lt;hdfs port&gt;<br />Rootpath: =/</p><p>If using the CLI, use the following entries in the Property file:<br />type=hdfs<br />host=&lt;hostname&gt;<br />rootPath=&lt;rootpath&gt;<br />port=&lt;port&gt;</p><h1 id="DataStores-HDFS2Datastore">HDFS2 Datastore</h1><p><br />HDFS2 datastore is used to connect to HDFS that comply with Apache Hadoop 2.0.2. This type of datastore has the same restrictions as the HDFS datastore</p><p>Properties</p><div class="table-wrap"><table class="confluenceTable"><tbody><tr><th class="confluenceTh">Property</th><th class="confluenceTh">Value</th></tr><tr><td class="confluenceTd">type</td><td class="confluenceTd">hdfs2</td></tr><tr><td class="confluenceTd">host</td><td class="confluenceTd"><p>The namenode server of the HDFS cluster.</p></td></tr><tr><td class="confluenceTd">port</td><td class="confluenceTd"><p>The namenode portof the HDFS cluster.The Pivotal HD default port is 8020.</p>.</td></tr><tr><td class="confluenceTd">rootPath</td><td class="confluenceTd">The starting/base of the HDFS namespace..</td></tr></tbody></table></div><p>If using the Web GUI, use the following values for the HDFS2 Datastore.</p><p>Type: hdfs2<br />Host: &lt;hdfs namenode host&gt;<br />Port: &lt;hdfs port&gt;<br />Rootpath: =/</p><p>If using the CLI, use the following entries in the Property file:</p><p>type=hdfs2<br />host=&lt;hostname&gt;<br />rootPath=&lt;rootpath&gt;<br />port=&lt;port&gt;</p><h2 id="DataStores-HowsetupanNFSserverandclient">How set up an NFS server and client</h2><h3 id="DataStores-SetuptheNFSserver">Set up the NFS server</h3><p>Run the following command to install NFS:</p><p># yum install nfs-utils nfs-utils-lib</p><p>Run the following commands to start NFS:</p><p># cd /etc/init.d<br /># ./rpcbind start<br /># ./nfs start</p><p>Create the NFS directory on the local machine.</p><p># mkdir /mnt<br /># mkdir /mnt/nfs</p><p>Edit the /etc/exports file and enter the paths to be accessed by all clients. Use hdsh* for all clients whose hostname starts with hdsh.<br />Open the /etc/exports file for editing:</p><p># vi /etc/exports</p><p>Enter the following content in the /etc/exports file:</p><p>/mnt/nfs hdsh*(rw,all_squash,sync,fsid=0)</p><p>Exit the file and run the command:</p><p># exportfs -ra</p><h3 id="DataStores-SetuptheNFSClient">Set up the NFS Client</h3><p>You must install the NFS Client on all DataLoader worker nodes.  To install the NFS client, enter:</p><p># yum install nfs-utils nfs-utils-lib</p><p>Start NFS:</p><p># cd /etc/init.d<br /># ./rpcbind start<br /># ./nfs start</p><p>Create the NFS directory:</p><p># mkdir /mnt<br /># mkdir /mnt/nfs</p><p>Mount the NFS directory.</p><p><strong>Note</strong>: NFS should be mounted on all the nodes (master, workers) on the same path.</p><p>Mount the client directory to the NFS server directory.</p><p>mount -t nfs &lt;nfs-server&gt;:&lt;server directory to be shared with all nfs clients&gt; &lt;client directory to be mounted&gt;</p><p>Execute the following command on all DataLoader nodes:</p><p># mount -t nfs &lt;nfs-server&gt;:/mnt/nfs /mnt/nfs</p><h1 id="DataStores-HowtosetuptheFTPserverandclient">How to set up the FTP server and client</h1><p>Install vsftpd:</p><p># yum install vsftpd<br /># chkconfig vsftpd on<br /># service vsftpd start<br /># service iptables start</p><p>Add the iptables entry for port 21.</p><p>Edit the iptables.<br /># vi /etc/sysconfig/iptables</p><p>Add an entry in /etc/sysconfig/iptables for port 21 as shown below.<br /><strong>Note</strong>: The entry should be added before rejecting related lines.</p><p>-A INPUT -m state --state NEW -m tcp -p tcp --dport 21 -j ACCEPT</p><p>Close the firewall on the server.<br /># /etc/init.d/iptables stop</p><p>Install FTP.<br /># yum install ftp</p><p>Add the ftp user on the Linux machine:<br /># useradd ftpuser<br /># passwd ftpuser</p><p>Add the same user (ftpuser) for ftp access. Open /etc/vsftpd/chroot_list for editing:</p><p>vi /etc/vsftpd/chroot_list</p><p>Add the ftp user name ftp user to the file:</p><p>[root@centos-1 \~\]# cat /etc/vsftpd/chroot_list<br /> ftpuser</p><p>Restart the service.</p><p># service vsftpd restart</p><p>Install the ftp Client.<br />yum install ftp</p><p><strong>Note</strong>: Because ftpuser was used to set up ftp service, the root folder is at /home/ftpuser.</p><p>The ftp Client must be installed on all DataLoader Worker nodes. Files must be created in the /home/ftpuser directory on the ftp server for DataLoader to find and upload them.</p><h1 id="DataStores-DatastoreConfigurationExamples">Datastore Configuration Examples</h1><p>The following datastore examples have been tested.</p><h2 id="DataStores-HDFSv1configuration">HDFS v1 configuration</h2><p>type=hdfs<br />host=hdsh070<br />port=54310<br />rootPath=/</p><h2 id="DataStores-HDFSv2configuration">HDFS v2 configuration</h2><p>type=hdfs2<br />host=hdsh070<br />port=8020<br />rootPath=/</p><h2 id="DataStores-NFSsetupandconfiguration">NFS setup and configuration</h2><p>type=nfs<br />host=hdsh070<br />rootPath=/<br />mountPoint=/mnt/nfs</p><h2 id="DataStores-FTPserversetup/configuration">FTP server setup/configuration</h2><p>type=ftp<br />host=hdsh070<br />port=21<br />rootPath=/pub<br />username=anonymous<br />password=guest</p><h2 id="DataStores-LocalFSenvironmentsetup">Local FS environment setup</h2><p>type=localfs<br />host=hdsh076<br />rootpath=/<br />username=test_user<br />password=test_user</p><p> </p><p><br /><br /></p>
                    </div>

                    
                                                      
                </div>             </div> 
            <div id="footer" style="background: url(http://confluence.greenplum.com/images/border/border_bottom.gif) repeat-x;">
                <p><small>Document generated by Confluence on Sep 23, 2013 15:58</small></p>
            </div>
        </div>     </body>
</html>
